# cera-vLLM Configuration
# Copy this file to .env and edit as needed: cp .env.example .env

# Dashboard password (required — change this!)
DASHBOARD_PASSWORD=changeme

# Optional: HuggingFace token for gated models (Llama, etc.)
# Get yours at https://huggingface.co/settings/tokens
HF_TOKEN=

# Port to expose vLLM server (default: 8100)
VLLM_PORT=8100

# Dashboard port (default: 8101)
DASHBOARD_PORT=8101

# Optional: Extra vLLM arguments (applied when model is activated)
# Examples:
#   --tensor-parallel-size 2       (split model across 2 GPUs)
#   --tensor-parallel-size 4       (split model across 4 GPUs)
#   --quantization awq             (AWQ quantization — lower VRAM)
#   --max-model-len 4096           (limit context length)
#   --gpu-memory-utilization 0.85  (reduce VRAM reservation)
VLLM_EXTRA_ARGS=

# Optional: Limit model download speed in KB/s (keeps SSH responsive)
# Examples:
#   50000   (~50 MB/s — good for 1 Gbit/s connections)
#   10000   (~10 MB/s — conservative, very stable SSH)
# Leave empty for unlimited speed
DOWNLOAD_SPEED_LIMIT=
